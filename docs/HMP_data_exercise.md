HMP Data Exercise
<a name="top"></a>
This document walks you through the steps of extracting data from the HMP Cloud Archive and completing the following analysis tasks.

1. [Compare 16S and WGS community profiles for a body site](#compare_16s_wgs)  
  1.1. [Download Qiime and MetaPhlAn2 community profiles](#download_profiles)  
  1.2. [Create random subset of samples for the body site](#create_random_subsamples)  
  1.3. [Extract matrices for subsamples](#extract_wgs_16s_matrices)  
  1.4. [Run Metaviz to visualize and compare the profiles](#run_metaviz_for_wgs_16s)
2. [Compare the 16S community profiles for two different body sites](#compare_16s_across_sites)  
  2.1. [Create random subset of samples for two body sites](#create_random_subsamples_for_two_sites)  
  2.2. [Extract matrices for subsamples](#extract_two_site_matrices)  
  2.3. [Run Metaviz to visualize and compare the profiles](#run_metaviz_for_two_sites)  
3. [Analyze 16S and WGS community profiles for a body site](#analyze_16s_wgs)  
  3.1. [Download the 16S trimmed sequence and WGS data from Cloud repository](#download_16s_wgs_data)  
  3.2. [Launch workflows to analyze downloaded data](#launch_16s_wgs_analysis)
4. [Analyze the 16S community profiles for two different body sites](#analyze_16s_across_sites)  
  4.1. [Download the 16S trimmed sequence from Cloud repository](#download_16s_data)  
  4.2. [Launch workflows to analyze downloaded data](#launch_16s_analysis)

Before you can start any of these exercises please launch the Docker image for the hmp_client using the following command and set the environment variable EX_SCRIPTS where some of the scripts exist:

### <a name="run_hmp_client_interactive"></a>Run HMP Interactive Docker Image
```
Chiron/bin/hmp_client_interactive
```

This will launch the Docker image with the tools necessary for the exercises. Once inside the Docker image set the environment variable necessary to execute some of the exercise scripts.

```
export EX_SCRIPTS=/tutorials/hmp_client/
```
### <a name="hmp_client"></a>HMP Client
For the exercises you will be using  data that is stored on the cloud. To make the data downloads easy we have written a tool, the [HMP data transfer tool](https://github.com/IGS/hmp_client), which has already been installed on your machine, to download files from Amazon Cloud to your machine. The data transfer tool works on a manifest file that you can generate (for this exercise we have pre generated the manifest files) using the [HMP Query interface](http://portal.ihmpdcc.org).

The usage of the <em>hmp_client</em> tool is described below:

```
Usage: hmp_client
    [-h] (-manifest MANIFEST | -url URL | -token TOKEN)
    [-destination DESTINATION]
    [-endpoint_priority ENDPOINT_PRIORITY]
    [-block_size BLOCK_SIZE]
    [-retries RETRIES]

```
[top](#top)

## <a name="compare_16s_wgs"></a>1. Compare 16S and WGS community profiles for a body site

The comparison of the 16S and corresponding WGS data is completed using the community profiles generated by Qiime and MetaPhlAn2 at the genus level. In this exercise you will download the precomputed community profiles, generate a list of a subset of samples for a particular body site and visit where we have both 16S and WGS data, and then extract the community profiles for this subset of samples. Once you have these subset profiles you will import them into Metaviz to compare the profiles graphically.


### <a name="download_profiles"></a>1.1. Download Qiime and MetaPhlAn community profiles
In the data_exercise directory you will find a manifest file which has the information that the GDC Client needs to download the precomputed community profiles. Copy the manifest file to the local working directory and run the following command to download the profiles:

```
mkdir /output/ex1
cd /output/ex1
cp /tutorials/hmp_client/community_profiles_manifest.tsv .
hmp_client -endpoint_priority S3,HTTP -manifest community_profiles_manifest.tsv -destination /output/ex1
gunzip ./otu_table_psn_v35.txt.gz
bunzip2 hmp1-II_metaphlan2-mtd-qcd.pcl.bz2
```

This should download the following files:  
    hmp1-II_metaphlan2-mtd-qcd.pcl.txt  
    otu_table_psn_v35.txt  

### <a name="create_random_subsamples"></a>1.2. Create random subset of samples for the body site
In the data exercise directory you will find the files that have the metadata associated with 16S and WGS data. Copy the two files (16s_metadata.tsv, wgs_metadata.tsv) to the local working directory.

```
cp /tutorials/hmp_client/16s_metadata.tsv .
cp /tutorials/hmp_client/wgs_metadata.tsv .
```

<a name="hmp_bodysites"></a>The HMP samples were collected from the following body sites:
```
Anterior_nares
Buccal_mucosa
Hard_palate
Keratinized_gingiva
L_Retroauricular_crease
Mid_vagina
Palatine_Tonsils
Posterior_fornix
R_Antecubital_fossa
R_Retroauricular_crease
STSite
Saliva
Stool
Subgingival_plaque
Supragingival_plaque
Throat
Tongue_dorsum
Vaginal_introitus
```
For this exercise we will use samples from the first visit for the following three body sites: "Anterior_nares", "Stool", and "Posterior_fornix".

[top](#top)

Use the script <em>generate_matched_visit_samples.R</em> to create a list of randomized samples from a particular body site and visit where we have both the 16S and WGS data. The usage of the script is described below.

```
Usage:
generate_matched_visit_samples.R
    -[-m16s|s] <16S metadata file>
    -[-wgs|w] <WGS metadata file>
    -[-bodysite|b] <bodysite>
    -[-visit|v] <visit number>
    -[-count|c] <rand subject count>
    -[-m16s_list|m] <16S sample_list>
    -[-wgs_list|g] <WGS sample list>
    [-[-help|h]]

```
The following command will extract matched samples for 25 randomly selected subjects from the Stool samples. The output is written to the specified files.

```
Rscript $EX_SCRIPTS/generate_matched_visit_samples.R --wgs wgs_metadata.tsv \
  --m16s 16s_metadata.tsv --count 25 --visit 1 --bodysite Stool \
  --16s_list stool_16s_rand_samples.tsv --wgs_list stool_wgs_rand_samples.tsv
```

#### <a name="extract_wgs_16s_matrices"></a>1.3. Extract matrices for subsamples
Use the script extract_subset.R to take the list of samples identified in the previous step and the community profiles generated by Qiime and MetaPhlAn2 and extract out the profiles for these matched samples.


##### Extract the Qiime subset
```
Usage:
extract_qiime_subset.R
    -[-qiime|q] <Qiime OTU table>
    -[-samples|s] <Samples file from previous step>
    -[-outfile|o] <Output file>
    [-[-help|h]]
```

The following command will generate the abundance matrices for the specified subset of samples.
```
Rscript $EX_SCRIPTS/extract_qiime_subset.R --qiime otu_table_psn_v35.txt \
  --samples stool_16s_rand_samples.tsv --outfile stool_16s_qiime.csv
```

##### Extract the MetaPhlAn2 subset
```
Usage:
extract_metaphlan_subset.R
    -[-metaphlan|m] <MetaPhlAn2 Abundance Table>
    -[-samples|s] <Samples file from previous step>
    -[-outfile|o] <Output file>
    [-[-help|h]]
```

The following command will generate the abundance matrices for the specified subset of samples.
```
Rscript $EX_SCRIPTS/extract_metaphlan_subset.R --metaphlan hmp1-II_metaphlan2-mtd-qcd.pcl \
  --samples stool_wgs_rand_samples.tsv --outfile stool_wgs_metaphlan.csv
```

#### <a name="run_metaviz_for_wgs_16s"></a>1.4. Run Metaviz to visualize and compare the profiles
Now that you have generated the abundance matrices for the subset of samples generated by Qiime and MetaPhlAn2 you can use Metaviz to visualize and compare these two matrices.

Before you can proceed to the visualization you will need to exit from the hmp_client_interactive Docker image and run the Metaviz Docker image by using the following commands.

```
exit
```

Please follow the instructions for using Metaviz to compare these generated files using the link below:  
[Compare 16S and WGS using Metaviz](https://github.com/IGS/Chiron/blob/master/docs/step-by-step-metavizr.Rmd)
[Top](#top)

## <a name="compare_16s_across_sites"></a>2. Compare the 16S community profiles for two different body sites

The comparison of the 16S data across two body sites is completed using the community profiles generated by Qiime. In this exercise you will download the precomputed community profiles, generate a list of a subset of samples for the two body sites and a particular visit, and then extract the community profiles for this subset of samples. Once you have these subset profiles you will import them into Metaviz to compare the profiles graphically.

#### Run the HMP Interactive Client Docker Image
```
bin/hmp_client_interactive
```

#### Download Qiime Community Profiles
You should already have the community profiles downloaded from the previous exercise, if not see the section [Download Qiime and MetaPhlAn Community Profiles](#download_profiles) to download the files.

### <a name="create_random_subsamples_for_two_sites"></a>2.1. Create random subset of samples for two body sites
In the data_exercise directory you will find the files that have the metadata associated with 16S samples. Copy the file (16s_metadata.tsv) to the local working directory. For this exercise we will use samples from the first visit for the following two body sites: "Anterior_nares" and "Stool".

```
mkdir /output/ex2
cd /output/ex2
cp /tutorials/hmp_client/16s_metadata.tsv .
```

Use the script <em>generate_matched_two_site_samples.R</em> to create a list of randomized samples from two body sites and a particular visit. The usage of the script is described below.

```
Usage:
generate_matched_visit_samples.R
    -[-outfile|o] <character>
    -[-bodysite1|a] <character>
    -[-bodysite2|b] <character>
    -[-region|r] <character>
    -[-visit|v] <visit number>
    -[-count|c] <rand subject count>
    [-[-help|h]]

```
The following command will extract matched samples for 25 randomly selected subjects from the Stool samples. The output is written to the file bodysite_rand_samples.txt

```
Rscript $EX_SCRIPTS/generate_matched_two_site_samples.R --m16s 16s_metadata.tsv --visit 1 \
  --count 20 --bodysite1 Stool --bodysite2 Anterior_nares \
  --outfile  stool_nares_subsamples.tsv \
  --region V35
```

#### <a name="extract_two_site_matrices"></a>2.2. Extract matrices for subsamples
Use the script extract_subset.R to take the list of samples identified in the previous step and the community profiles generated by Qiime and MetaPhlAn2 and extract out the profiles for these matched samples.

```
Usage:
extract_qiime_subset.R
    -[-qiime|q] <Qiime OTU table>
    -[-samples|s] <Samples file from previous step>
    -[-outfile|o] <character>    
    [-[help|h]]
```

The following command will generate the abundance matrices for the specified subset of samples.
```
Rscript $EX_SCRIPTS/extract_qiime_subset.R --qiime ../ex1/otu_table_psn_v35.txt \
  --samples stool_nares_subsamples.tsv --outfile stool_nares_subsamples_otu_table_psn_v35.txt
```

#### <a name="run_metaviz_for_two_sites"></a>2.3. Run Metaviz to compare the abundance matrices for two sites
Now that you have generated the abundance matrices for the subset of samples generated by Qiime you can use Metaviz to visualize and compare these two matrices.

Before you can proceed to the visualization you will need to exit from the hmp_client_interactive Docker image and run the Metaviz Docker image by using the following commands.

```
exit
bin/metaviz_interactive
```

[top](#top)
## <a name="analyze_16s_wgs"></a>3. Analyze 16S and WGS community profiles for a body site

In this part of the exercise you will first analyze the 16S and WGS data using Qiime and MetaPhlAn2 tools. You will then take the results from this analysis and compare them at the genus level using Metaviz like the way you did in the previous exercise.


### Run the HMP Interactive Client Docker Image
```
bin/hmp_client_interactive
```

###  <a name="download_16s_wgs_data"></a>3.1. Download the 16S trimmed sequence and WGS data from Cloud repository
In the data_exercise directory you will find the files that have the metadata associated with 16S samples. For this exercise we will use samples from the first visit for the following two body sites: "Anterior_nares" and "Stool". Copy the manifest files <em>stool_rand_wgs_manifest.tsv</em> and <em>stool_rand_16s_manifest.tsv</em> in the examples directory to the local directory and use the script <em>hmp_client</em> to download the data files specified in the manifest file.

```
mkdir /output/ex3
cd /output/ex3
cp /tutorials/hmp_client/stool_wgs_rand_5_samples_manifest.tsv .
cp /tutorials/hmp_client/stool_16s_rand_5_samples_manifest.tsv .
```

The following commands will download trimmed 16S sequences and the corresponding WGS samples for the 5 randomly selected subject visits to the directory specified by the <em>destination</em> parameter. Once the data has been downloaded exit the current Docker image.

```
hmp_client  -endpoint_priority S3,HTTP -manifest stool_16s_rand_5_samples_manifest.tsv -destination stool_16s
hmp_client  -endpoint_priority S3,HTTP -manifest stool_wgs_rand_5_samples_manifest.tsv -destination stool_wgs
exit
```


###  <a name="launch_16s_wgs_analysis"></a>3.2. Launch workflows to analyze downloaded data
To make the usage of the Docker images and for the ease of the exercises, we have built simple scripts that can create workflows defined in the Common Workflow Language (CWL). These workflows can be executed using the <em>cwl-runner</em>, a command-line tool to execute the workflows. The workflow runner uses the predefined Docker containers in batch modes to complete the analysis tasks. You can find workflows for all the tools used in this workshop including Qiime, HUMAnN2, MetaCompass, and StrainPhlAn.

[top](#top)

#### Launch workflow to analyze the 16S data using Qiime
```
Usage:
usage: qiime2_pipeline
    [-h]
    --input_dir /path/to/input/dir
    --config_file /path/to/qiime2_config.yml
    [--out_dir /path/to/outdir]
```
Before running the command, the 'qiime2_config_template' file needs to be copied and any necessary parameters, such as various file paths or category names need to be filled in.  Some parameters are left filled in as default settings.  This complete copy (let's call it stool_16s_config.yml for this example) will be used to specify parameters for the pipeline.

The following command will run the QIIME2 process on all the files in the specified data directory.

```
create_qiime_workflow --input_dir stool_16s --config_file stool_16s_config.yml -out_dir stool_16s_results
```

This workflow will process the individual files in the specified data directory and write the individual OTU tables. It will then create a combined OTU table for all the samples.


[top](#top)
#### Launch the workflows to analyze the WGS data using HUMAnN2
```
Usage:
usage: humann2_pipeline
    [-h]
    --input_file_list /path/to/input.list
    --config_file /path/to/qiime2_config.yml
    [--out_dir /path/to/outdir]
```
Before running the command, the 'humann2_config_template' file needs to be copied and any necessary parameters, such as various file paths or category names need to be filled in.  Some parameters are left filled in as default settings.  This complete copy (let's call it stool_16s_config.yml for this example) will be used to specify parameters for the pipeline.

The following command will run the HUMAnN2 process on all the files in the specified input file list, one file per line.

```
humann2_pipeline --input_file_list stool_wgs.list --config_file stool_16s_config.yml -out_dir stool_wgs_results
```

This workflow will process the individual files in the specified input file list and write the individual MetaPhlAn2 and HUMAnN2 tables. It will then create a combined relative abundance table for all the samples based on MetaPhlAn2 results.

[top](#top)
## <a name="analyze_16s_wgs"></a>4. Analyze the 16S community profiles for two different body sites

In this part of the exercise you will first analyze the 16S data for the two body sites using Qiime. You will take the results from this analysis and then visualize and compare the data generated for the two body sites using Metaviz.

###   <a name="download_16s_data"></a>4.1. Download the 16S trimmed sequence from Cloud repository
In the data_exercise directory you will find the files that have the metadata associated with 16S samples. For this exercise we will use samples from the first visit for the following two body sites: "Anterior_nares" and "Stool". Copy the manifest files <em>stool_nares_rand_16s_manifest.tsv</em> in the examples directory to the local directory and use the script <em>hmp_client</em> to download the data files specified in the manifest file.

```
mkdir /output/ex4
cd /output/ex4
cp /tutorials/hmp_client/stool_nares_16s_rand_5_samples_manifest.tsv .
```

The following commands will download trimmed 16S sequences for the 5 randomly selected subject visits to the directory specified by the <em>destination</em> parameter. Once the data has been downloaded exit the current Docker image.

```
hmp_client  -endpoint_priority S3,HTTP -manifest stool_nares_16s_rand_5_samples_manifest.tsv \
  -destination stool_nares_16s
exit
cd /opt/chiron/hmp_client/ex4
```

###   <a name="launch_16s_analysis"></a>4.2.  Launch workflows to analyze downloaded data
To make the usage of the Docker images and for the ease of the exercises, we have built simple scripts that can create workflows defined in the Common Workflow Language (CWL). These workflows can be executed using the <em>cwl-runner</em>, a command-line tool to execute the workflows. The workflow runner uses the predefined Docker containers in batch modes to complete the analysis tasks. You can find workflows for all the tools used in this workshop including Qiime, HUMAnN2, MetaCompass, and StrainPhlAn.

[top](#top)

#### Launch workflow to analyze the 16S data using Qiime
```
Usage:
usage: qiime2_pipeline
    [-h]
    --input_dir /path/to/input/dir
    --config_file /path/to/qiime2_config.yml
    [--out_dir /path/to/outdir]
```

Before running the command, the 'qiime2_config_template' file needs to be copied and any necessary parameters, such as various file paths or category names need to be filled in.  Some parameters are left filled in as default settings.  This complete copy (let's call it stool_16s_config.yml for this example) will be used to specify parameters for the pipeline.

The following command will run the QIIME2 process on all the files in the specified input directory.

```
qiime2_pipeline --input_dir stool_16s --config_file stool_16s_config.yml -out_dir stool_16s_results
```

This workflow will process the individual files in the specified data directory and write the individual OTU tables. It will then create a combined OTU table for all the samples.

[top](#top)
# Related Links:

- [Discussion group site](https://groups.google.com/forum/#!forum/hmp-cloud-pilot) (Google groups, for collaborators)
- [HMP Query interface](http://portal.ihmpdcc.org) - Click 'data' to get the facet search
- [HMP Client](https://github.com/IGS/hmp_client) - HMP Data Transfer Tool is a command line client to download data from the HMP data repository to local machine using a manifest file generated through the query interface.
- [Common Workflow Language](http://www.commonwl.org/draft-3/index.html) - The common workflow language used in Chiron

[top](#top)
